{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Input"
      ],
      "metadata": {
        "id": "M3SFKWWWZMkn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HymHal9_VaLs"
      },
      "outputs": [],
      "source": [
        "paragraph=\"\"\"My dear young friends, dream, dream, dream. Dreams transform into thoughts and thoughts result in action. You have to dream before your dreams can come true. You should have a goal and a constant quest to acquire knowledge. Hard work and perseverance are essential. Use technology for the benefit of humankind and not for its destruction. The ignited mind of the youth is the most powerful resource on the earth, above the earth, and under the earth. When the student is ready, the teacher will appear. Aim high, dream big, and work hard to achieve those dreams. The future belongs to the young who have the courage to dream and the determination to realize those dreams. Remember, small aim is a crime; have great aim and pursue it with all your heart.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#StopWords:\n",
        "Stopwords are common words that are often removed from text data in natural language processing (NLP) tasks because they are considered to have little to no meaningful information for certain applications. Examples of stopwords include words like \"a,\" \"an,\" \"the,\" \"and,\" \"but,\" \"or,\" \"on,\" \"in,\" and \"with.\"\n",
        "\n",
        "**Why Remove Stopwords?**\n",
        "\n",
        "\n",
        "1.  **Reduce Data Size:**Removing stopwords can reduce the size of the dataset, making processing more efficient.\n",
        "2.   **Improve Model Performance:** For tasks like text classification or clustering, removing stopwords can help improve the performance of the model by reducing noise.\n",
        "3.   **Focus on Meaningful Words:** Stopwords are generally considered to add little value in understanding the context or meaning of a document.\n",
        "\n"
      ],
      "metadata": {
        "id": "2ZNv9FxWav_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StopWords Techniques:"
      ],
      "metadata": {
        "id": "ydUFovgQcgNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.StopWords using NLTK"
      ],
      "metadata": {
        "id": "U0b-AxbgcmrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIh0jWIXV6sO",
        "outputId": "025b35aa-ba88-41bf-e9c7-3ced0c66ad2e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc42PaZ8V9s-",
        "outputId": "6c2ab266-049f-4630-d8f1-b480fcfaa061"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the stopwords list in English Language."
      ],
      "metadata": {
        "id": "Ru3dkJGCcvNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJR31s4tWLWF",
        "outputId": "ba5f82f5-fcd7-4482-b950-a49f85afe39d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QPFOz6idie3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "words=word_tokenize(paragraph)\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHSEImr2Wlpl",
        "outputId": "ea1d3ff8-ea0f-4c78-eeed-cf7907e118e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'dear', 'young', 'friends', ',', 'dream', ',', 'dream', ',', 'dream', '.', 'Dreams', 'transform', 'into', 'thoughts', 'and', 'thoughts', 'result', 'in', 'action', '.', 'You', 'have', 'to', 'dream', 'before', 'your', 'dreams', 'can', 'come', 'true', '.', 'You', 'should', 'have', 'a', 'goal', 'and', 'a', 'constant', 'quest', 'to', 'acquire', 'knowledge', '.', 'Hard', 'work', 'and', 'perseverance', 'are', 'essential', '.', 'Use', 'technology', 'for', 'the', 'benefit', 'of', 'humankind', 'and', 'not', 'for', 'its', 'destruction', '.', 'The', 'ignited', 'mind', 'of', 'the', 'youth', 'is', 'the', 'most', 'powerful', 'resource', 'on', 'the', 'earth', ',', 'above', 'the', 'earth', ',', 'and', 'under', 'the', 'earth', '.', 'When', 'the', 'student', 'is', 'ready', ',', 'the', 'teacher', 'will', 'appear', '.', 'Aim', 'high', ',', 'dream', 'big', ',', 'and', 'work', 'hard', 'to', 'achieve', 'those', 'dreams', '.', 'The', 'future', 'belongs', 'to', 'the', 'young', 'who', 'have', 'the', 'courage', 'to', 'dream', 'and', 'the', 'determination', 'to', 'realize', 'those', 'dreams', '.', 'Remember', ',', 'small', 'aim', 'is', 'a', 'crime', ';', 'have', 'great', 'aim', 'and', 'pursue', 'it', 'with', 'all', 'your', 'heart', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_after_stopwords=[]\n",
        "word_as_stopwords=[]\n",
        "for word in words:\n",
        "  if word not in set (nltk.corpus.stopwords.words('english')):\n",
        "    words_after_stopwords.append(word)\n",
        "  else:\n",
        "    word_as_stopwords.append(word)\n",
        "\n",
        "print(\"Words after removing stopwords:\", words_after_stopwords)\n",
        "print(\"Stopwords:\",word_as_stopwords)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88poZuIiYBCP",
        "outputId": "91948b0e-8471-4828-eea8-8dff2a3a6414"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words after removing stopwords: ['My', 'dear', 'young', 'friends', ',', 'dream', ',', 'dream', ',', 'dream', '.', 'Dreams', 'transform', 'thoughts', 'thoughts', 'result', 'action', '.', 'You', 'dream', 'dreams', 'come', 'true', '.', 'You', 'goal', 'constant', 'quest', 'acquire', 'knowledge', '.', 'Hard', 'work', 'perseverance', 'essential', '.', 'Use', 'technology', 'benefit', 'humankind', 'destruction', '.', 'The', 'ignited', 'mind', 'youth', 'powerful', 'resource', 'earth', ',', 'earth', ',', 'earth', '.', 'When', 'student', 'ready', ',', 'teacher', 'appear', '.', 'Aim', 'high', ',', 'dream', 'big', ',', 'work', 'hard', 'achieve', 'dreams', '.', 'The', 'future', 'belongs', 'young', 'courage', 'dream', 'determination', 'realize', 'dreams', '.', 'Remember', ',', 'small', 'aim', 'crime', ';', 'great', 'aim', 'pursue', 'heart', '.']\n",
            "Stopwords: ['into', 'and', 'in', 'have', 'to', 'before', 'your', 'can', 'should', 'have', 'a', 'and', 'a', 'to', 'and', 'are', 'for', 'the', 'of', 'and', 'not', 'for', 'its', 'of', 'the', 'is', 'the', 'most', 'on', 'the', 'above', 'the', 'and', 'under', 'the', 'the', 'is', 'the', 'will', 'and', 'to', 'those', 'to', 'the', 'who', 'have', 'the', 'to', 'and', 'the', 'to', 'those', 'is', 'a', 'have', 'and', 'it', 'with', 'all', 'your']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.StopWords using Spacy\n"
      ],
      "metadata": {
        "id": "d7fali5Ac9u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Spacy\n",
        "!python -m spacy download en\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqpg8fzZdYEr",
        "outputId": "aecfe643-24b3-4bb4-f53d-6d1878f145f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from Spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from Spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from Spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from Spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from Spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from Spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from Spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from Spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from Spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from Spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from Spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from Spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from Spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from Spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from Spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from Spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from Spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from Spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from Spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->Spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->Spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->Spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->Spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->Spacy) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->Spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->Spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->Spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->Spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->Spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->Spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->Spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->Spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->Spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->Spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->Spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->Spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->Spacy) (0.1.2)\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the English model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Access the stopwords\n",
        "stop_words = nlp.Defaults.stop_words\n",
        "\n",
        "print(stop_words)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWkVZ6mbW69T",
        "outputId": "ffd6b834-3be4-452b-fa69-6fabb0e31037"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'too', 'used', 'twenty', 'front', 'ten', 'former', '’re', 'hence', 'four', 'meanwhile', 'nobody', 'at', 'in', 'so', 'hereupon', 'please', 'back', 'their', 'until', 'is', 'once', 'being', 'seems', 'does', 'because', 'various', 'top', 'under', 'will', 'call', 'keep', 'ca', 'either', '’ll', 'two', 'yourselves', 'anyhow', 'noone', \"'re\", 'after', 'she', 'name', 'toward', 'around', 'other', 'first', 'before', 'which', 'few', 'less', 'beforehand', 'otherwise', 'no', 'several', 'namely', 'they', 'not', 'bottom', 'really', '‘m', 'enough', 'made', 'very', 'whereafter', 'thus', 'six', 'do', 'over', 'has', 'against', 'anyway', 'though', 'whereupon', '‘ve', 'afterwards', 'now', 'be', 'make', 'say', 'neither', 'empty', 'together', 'some', 'last', 'seeming', 'put', 'are', 'would', 'myself', 'anywhere', 'became', 'him', 'next', 'becoming', 'hundred', 'whom', 'our', 'was', 'done', 'get', 'another', 'besides', 'anything', 'those', 'except', 'serious', 'such', '‘s', 'on', 'but', 'herein', 'hers', 'move', 'everywhere', 'latterly', 'a', 'ours', 'more', 'how', 'therein', 'again', 'somehow', 'through', 'about', 'your', 'that', 'even', 'five', 'nine', 'onto', 'since', 'have', 'both', '’d', 'nothing', 'might', 'itself', 'down', 'wherever', 'hereafter', 'been', 'take', \"'d\", 'should', 'beyond', 'many', 'nor', 'up', 'unless', 'latter', 'still', 'therefore', 'whenever', 'with', 'its', 'he', 'my', 'here', 'although', 'can', 'also', 'per', 'thereupon', 'what', 'yet', 'however', '‘re', 'could', 'am', 'between', 'see', 'go', 'hereby', 'becomes', 'yours', 'we', 'above', 'alone', 'there', 'amount', 'thru', 'upon', 'fifteen', 'twelve', 'then', 'along', 'whereby', 'whither', 'show', \"'m\", 'whence', \"n't\", 'always', 'off', 'did', 'thence', 'her', 'part', 'someone', 'us', 'had', 'wherein', 'something', 'and', 'ever', 're', 'further', 'become', 'often', 'while', 'everything', 'forty', 'behind', 'seem', 'whatever', 'why', 'quite', 'due', 'regarding', 'all', 'eleven', 'third', 'fifty', 'same', 'who', 'an', 'it', 'nowhere', 'yourself', 'somewhere', 'give', 'may', 'than', 'among', 'below', 'never', 'cannot', 'elsewhere', '‘ll', 'to', 'n‘t', 'were', 'whoever', 'else', 'any', 'you', 'well', 'whose', 'by', 'the', '’m', 'thereafter', 'nevertheless', 'moreover', 'mine', 'during', 'via', 'across', 'three', \"'s\", 'into', 'indeed', 'from', 'sixty', 'every', 'perhaps', 'none', 'just', 'them', 'ourselves', 'me', 'where', '’s', 'herself', 'himself', 'or', 'of', 'whether', 'whereas', 'formerly', 'these', 'throughout', 'as', 'full', \"'ve\", 'already', 'own', 'anyone', 'whole', 'side', 'others', 'if', 'without', '’ve', 'least', 'themselves', 'for', 'n’t', 'only', 'doing', 'out', '‘d', 'amongst', 'his', 'towards', 'eight', 'everyone', 'beside', 'almost', 'sometimes', 'using', 'thereby', 'i', 'within', 'most', \"'ll\", 'when', 'much', 'sometime', 'seemed', 'mostly', 'this', 'rather', 'each', 'one', 'must'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the text\n",
        "doc = nlp(paragraph)\n",
        "\n",
        "# Remove stopwords\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7IWiomagXZe",
        "outputId": "fe31e108-21f5-4084-df9a-c74b878227b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dear', 'young', 'friends', ',', 'dream', ',', 'dream', ',', 'dream', '.', 'Dreams', 'transform', 'thoughts', 'thoughts', 'result', 'action', '.', 'dream', 'dreams', 'come', 'true', '.', 'goal', 'constant', 'quest', 'acquire', 'knowledge', '.', 'Hard', 'work', 'perseverance', 'essential', '.', 'Use', 'technology', 'benefit', 'humankind', 'destruction', '.', 'ignited', 'mind', 'youth', 'powerful', 'resource', 'earth', ',', 'earth', ',', 'earth', '.', 'student', 'ready', ',', 'teacher', 'appear', '.', 'Aim', 'high', ',', 'dream', 'big', ',', 'work', 'hard', 'achieve', 'dreams', '.', 'future', 'belongs', 'young', 'courage', 'dream', 'determination', 'realize', 'dreams', '.', 'Remember', ',', 'small', 'aim', 'crime', ';', 'great', 'aim', 'pursue', 'heart', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.StopWords using scikit-learn (CountVectorizer)\n"
      ],
      "metadata": {
        "id": "JLuH8CcthaT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5epuAVSuhfpn",
        "outputId": "f63e5e43-08ca-4d79-918b-2e4b320a686a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence Tokenization\n",
        "#Spliting the paragraph into sentences.\n",
        "paragraph=\"\"\"My dear young friends, dream, dream, dream. Dreams transform into thoughts and thoughts result in action. You have to dream before your dreams can come true. You should have a goal and a constant quest to acquire knowledge. Hard work and perseverance are essential. Use technology for the benefit of humankind and not for its destruction. The ignited mind of the youth is the most powerful resource on the earth, above the earth, and under the earth. When the student is ready, the teacher will appear. Aim high, dream big, and work hard to achieve those dreams. The future belongs to the young who have the courage to dream and the determination to realize those dreams. Remember, small aim is a crime; have great aim and pursue it with all your heart.\"\"\"\n",
        "tokens= paragraph.split('.')\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwpUHaH1ihpY",
        "outputId": "7f848666-f256-4675-f110-3d731fe65480"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My dear young friends, dream, dream, dream',\n",
              " ' Dreams transform into thoughts and thoughts result in action',\n",
              " ' You have to dream before your dreams can come true',\n",
              " ' You should have a goal and a constant quest to acquire knowledge',\n",
              " ' Hard work and perseverance are essential',\n",
              " ' Use technology for the benefit of humankind and not for its destruction',\n",
              " ' The ignited mind of the youth is the most powerful resource on the earth, above the earth, and under the earth',\n",
              " ' When the student is ready, the teacher will appear',\n",
              " ' Aim high, dream big, and work hard to achieve those dreams',\n",
              " ' The future belongs to the young who have the courage to dream and the determination to realize those dreams',\n",
              " ' Remember, small aim is a crime; have great aim and pursue it with all your heart',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the CountVectorizer with English stopwords\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the data\n",
        "X = vectorizer.fit_transform(tokens)\n",
        "\n",
        "# Get the feature names (i.e., the words that are not stopwords)\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR8GpFphhpUE",
        "outputId": "b796727b-d6c8-4e0f-d634-43b211424b11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['achieve' 'acquire' 'action' 'aim' 'appear' 'belongs' 'benefit' 'big'\n",
            " 'come' 'constant' 'courage' 'crime' 'dear' 'destruction' 'determination'\n",
            " 'dream' 'dreams' 'earth' 'essential' 'friends' 'future' 'goal' 'great'\n",
            " 'hard' 'heart' 'high' 'humankind' 'ignited' 'knowledge' 'mind'\n",
            " 'perseverance' 'powerful' 'pursue' 'quest' 'ready' 'realize' 'remember'\n",
            " 'resource' 'result' 'small' 'student' 'teacher' 'technology' 'thoughts'\n",
            " 'transform' 'true' 'use' 'work' 'young' 'youth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.StopWords using scikit-learn (TfidfVectorizer)\n",
        "\n",
        "TfidfVectorizer is used to remove stopwords and compute the TF-IDF representation."
      ],
      "metadata": {
        "id": "7Z1NBzhWR-38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TfidfVectorizer with English stopwords\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the data\n",
        "X = vectorizer.fit_transform(tokens)\n",
        "\n",
        "# Get the feature names (i.e., the words that are not stopwords)\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2sOCd1tVb-m",
        "outputId": "e7a434ae-db22-4610-d6c8-a4873c5f4b8a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['achieve' 'acquire' 'action' 'aim' 'appear' 'belongs' 'benefit' 'big'\n",
            " 'come' 'constant' 'courage' 'crime' 'dear' 'destruction' 'determination'\n",
            " 'dream' 'dreams' 'earth' 'essential' 'friends' 'future' 'goal' 'great'\n",
            " 'hard' 'heart' 'high' 'humankind' 'ignited' 'knowledge' 'mind'\n",
            " 'perseverance' 'powerful' 'pursue' 'quest' 'ready' 'realize' 'remember'\n",
            " 'resource' 'result' 'small' 'student' 'teacher' 'technology' 'thoughts'\n",
            " 'transform' 'true' 'use' 'work' 'young' 'youth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Customization of own StopWords:\n",
        "\n",
        "Customizing own stopwords in scikit-learn is straightforward. We can either modify the existing list of stopwords provided by scikit-learn or create an entirely new list.\n"
      ],
      "metadata": {
        "id": "3SQ2t2BFXGXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.Using Custom Stopwords with CountVectorizer"
      ],
      "metadata": {
        "id": "qY7p5BFNZ7QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text data\n",
        "texts = \"I don’t even know where to begin with you. It all started about a year ago when I first stumbled upon a photo of you somewhere on the internet.I was sure you were located somewhere super close, already preparing myself for a trip to Czechia. How wrong I was!\"\n",
        "#Sentence Tokenizer\n",
        "texts=texts.split('.')\n",
        "texts\n",
        "# Define custom stopwords\n",
        "custom_stop_words = ['this','to','it', 'is', 'an', 'of', 'the', 'with']\n",
        "# Initialize the CountVectorizer with custom stopwords\n",
        "vectorizer = CountVectorizer(stop_words=custom_stop_words)\n",
        "\n",
        "# Fit and transform the data\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Get the feature names (i.e., the words that are not stopwords)\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh3VIxePYHJR",
        "outputId": "95070d0e-bd99-44f7-e352-acd8e9ef16e6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['about' 'ago' 'all' 'already' 'begin' 'close' 'czechia' 'don' 'even'\n",
            " 'first' 'for' 'how' 'internet' 'know' 'located' 'myself' 'on' 'photo'\n",
            " 'preparing' 'somewhere' 'started' 'stumbled' 'super' 'sure' 'trip' 'upon'\n",
            " 'was' 'were' 'when' 'where' 'wrong' 'year' 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.Combining Default Stopwords with Custom Stopwords"
      ],
      "metadata": {
        "id": "sQZpSpOdd9mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "\n",
        "# Sample text data\n",
        "texts = \"I don’t even know where to begin with you. It all started about a year ago when I first stumbled upon a photo of you somewhere on the internet. I was sure you were located somewhere super close, already preparing myself for a trip to Czechia. How wrong I was!\"\n",
        "\n",
        "# Extend the default stopwords with custom ones\n",
        "custom_stop_words = ENGLISH_STOP_WORDS.union(['dont', 'with'])\n",
        "custom_stop_words = list(custom_stop_words)  # Convert to list   #without thiis line there will be a parameter error.\n",
        "\n",
        "# Initialize the CountVectorizer with combined stopwords\n",
        "vectorizer = CountVectorizer(stop_words=custom_stop_words)\n",
        "\n",
        "# Fit and transform the data\n",
        "X = vectorizer.fit_transform([texts])\n",
        "\n",
        "# Get the feature names (i.e., the words that are not stopwords)\n",
        "features = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si-oheI0eFPM",
        "outputId": "b4cf5c09-33a5-4454-8d4e-5cdec8e8267d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ago' 'begin' 'close' 'czechia' 'don' 'internet' 'know' 'located' 'photo'\n",
            " 'preparing' 'started' 'stumbled' 'super' 'sure' 'trip' 'wrong' 'year']\n"
          ]
        }
      ]
    }
  ]
}